import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._

val spark = SparkSession.builder().appName("Monthly Count").getOrCreate()
import spark.implicits._

// Define your paths and months
val paths = Seq("/hdfs/path/asset1", "/hdfs/path/asset2")
val months = Seq("2024-01", "2024-02") // adjust as needed
val queryLabel = "filter_active_users" // name of your query for reference

// Helper function to apply query
def applyQuery(df: DataFrame): DataFrame = {
  // Replace with your actual filtering logic
  df.filter($"status" === "active")
}

// Collect results
val results = for {
  path <- paths
  month <- months
  fullPath = s"$path/month=$month"
  if new java.io.File(fullPath).exists() // optional: skip if data missing
} yield {
  try {
    val df = spark.read.parquet(fullPath) // or .json/.csv based on format
    val filtered = applyQuery(df)
    val count = filtered.count()
    (path, month, queryLabel, count)
  } catch {
    case e: Exception =>
      println(s"Error processing $fullPath: ${e.getMessage}")
      (path, month, queryLabel, -1L)
  }
}

// Convert to DataFrame and write as CSV
val resultDF = results.toDF("path", "month", "query", "count")
resultDF.write.option("header", "true").csv("/your/output/path/result.csv")