filebeat.inputs:
  - type: log
    paths:
      - /usr/share/filebeat/hdfs-to-es-logs/*/*/*.json
    fields:
      log_type: "summary"  # For files in the summary subdir
    fields_under_root: true

    # Add grok processor to extract asset from the file path
    processors:
      - grok:
          patterns:
            - '%{GREEDYDATA:dir_type}/%{DATA:asset}/file-%{WORD:asset_name}-%{DATE:date}.json'
          on_failure:
            - drop_event

output.logstash:
  hosts: ["logstash:5044"]












input {
  beats {
    port => 5044
  }
}

filter {
  if [log_type] == "logs" {
    grok {
      match => { "message" => "%{GREEDYDATA:log_content}" }
    }
    mutate {
      add_field => { "[@metadata][index_prefix]" => "logs" }
    }
  } else if [log_type] == "summary" {
    json {
      source => "message"
    }
    mutate {
      add_field => { "[@metadata][index_prefix]" => "summary" }
    }
  }

  # Check if asset is present and use it to construct the index name
  mutate {
    add_field => { "[@metadata][index]" => "%{[@metadata][index_prefix]}-%{asset}" }
  }
}

output {
  elasticsearch {
    hosts => ["${ELASTIC_HOSTS}"]
    index => "%{[@metadata][index]}-%{+YYYY.MM.dd}"
    user => "${ELASTIC_USER}"
    password => "${ELASTIC_PASSWORD}"
  }

  stdout {
    codec => rubydebug
  }
}