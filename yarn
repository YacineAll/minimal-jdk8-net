#!/bin/bash

# Run start-spark.sh in the background and capture its output
output=$(sh start-spark.sh 2>&1 &)
pid=$!  # Get the PID of the background process

# Wait briefly to ensure output includes the Application ID
sleep 5

# Extract the Application ID from the captured output
application_id=$(echo "$output" | grep -o 'application_[0-9]*_[0-9]*')
if [[ -z "$application_id" ]]; then
  echo "Failed to retrieve Application ID."
  exit 1
fi

echo "Application ID: $application_id"

# Initialize job state
job_state=""

# Loop to monitor the Spark job status
while true; do
  # Query the job status using the Application ID
  job_state=$(yarn application -status "$application_id" 2>/dev/null | grep -o 'State : [A-Z]*' | awk '{print $3}')

  # Check the job state and handle each case
  if [[ "$job_state" == "RUNNING" ]]; then
    echo "Job is currently running..."
  elif [[ "$job_state" == "FINISHED" ]]; then
    echo "Job has finished successfully."
    break
  elif [[ "$job_state" == "KILLED" ]]; then
    echo "Job was killed."
    break
  elif [[ "$job_state" == "FAILED" ]]; then
    echo "Job has failed."
    break
  else
    echo "Job state: $job_state"
  fi

  # Wait a few seconds before checking again
  sleep 5
done